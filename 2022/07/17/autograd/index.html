<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="torch.autogradtorch.autograd是PyTorch自动求导的工具，求导支撑着神经网络的训练。 1. 背景神经网络（NN）是作用在输入数据上的一系列嵌套函数的集合，这些函数由参数（weights和biases）定义，被存储在PyTorch的tensor中。 训练神经网络的两个步骤  前向传播：在前向传播中，NN尽最大努力猜测正确的输出结果。如何猜测呢？将输入数据送入到NN中的每">
<meta property="og:type" content="article">
<meta property="og:title" content="60分钟入门PyTorch——自动求导autograd">
<meta property="og:url" content="http://example.com/2022/07/17/autograd/index.html">
<meta property="og:site_name" content="TT&#39;Blog">
<meta property="og:description" content="torch.autogradtorch.autograd是PyTorch自动求导的工具，求导支撑着神经网络的训练。 1. 背景神经网络（NN）是作用在输入数据上的一系列嵌套函数的集合，这些函数由参数（weights和biases）定义，被存储在PyTorch的tensor中。 训练神经网络的两个步骤  前向传播：在前向传播中，NN尽最大努力猜测正确的输出结果。如何猜测呢？将输入数据送入到NN中的每">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="c:/Users/TSY/Desktop/Snipaste_2022-07-17_21-23-00.png">
<meta property="og:image" content="https://pytorch.org/tutorials/_images/dag_autograd.png">
<meta property="article:published_time" content="2022-07-16T16:00:00.000Z">
<meta property="article:modified_time" content="2023-09-13T11:49:49.248Z">
<meta property="article:author" content="TT">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:/Users/TSY/Desktop/Snipaste_2022-07-17_21-23-00.png">

<link rel="canonical" href="http://example.com/2022/07/17/autograd/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>60分钟入门PyTorch——自动求导autograd | TT'Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TT'Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/07/17/autograd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="TT">
      <meta itemprop="description" content="The Journey Is the Reward">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TT'Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          60分钟入门PyTorch——自动求导autograd
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-07-17 00:00:00" itemprop="dateCreated datePublished" datetime="2022-07-17T00:00:00+08:00">2022-07-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-09-13 19:49:49" itemprop="dateModified" datetime="2023-09-13T19:49:49+08:00">2023-09-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9A-PyTorch/" itemprop="url" rel="index"><span itemprop="name">深度学习： PyTorch</span></a>
                </span>
            </span>

          
            <span id="/2022/07/17/autograd/" class="post-meta-item leancloud_visitors" data-flag-title="60分钟入门PyTorch——自动求导autograd" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/07/17/autograd/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/07/17/autograd/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="torch-autograd"><a href="#torch-autograd" class="headerlink" title="torch.autograd"></a>torch.autograd</h1><p><code>torch.autograd</code>是PyTorch自动求导的工具，求导支撑着神经网络的训练。</p>
<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>神经网络（NN）是作用在<strong>输入数据</strong>上的一系列嵌套函数的集合，这些函数由参数（weights和biases）定义，被存储在PyTorch的tensor中。</p>
<p>训练神经网络的两个步骤</p>
<ul>
<li><strong>前向传播</strong>：在前向传播中，NN尽最大努力猜测正确的输出结果。如何猜测呢？将输入数据送入到NN中的每一个函数进行处理。</li>
<li><strong>反向传播</strong>：在反向传播中，NN根据上一步骤猜测的误差来相应地调整参数。如何调整的？它通过从输出结果向回遍历，收集有关函数参数（梯度）的误差的导数，并使用梯度下降来优化参数。更多的细节可以参照3Blue1Brown的讲解视频。</li>
</ul>
<hr>
<h2 id="2-在PyTorch中的应用"><a href="#2-在PyTorch中的应用" class="headerlink" title="2. 在PyTorch中的应用"></a>2. 在PyTorch中的应用</h2><p>下面看一个示例的训练步骤，从torchvision中加载一个预训练的<strong>resnet18</strong></p>
<p><strong>模型</strong>。</p>
<p>使用随机数创建tensor去表示一张图片：有3个通道，高和宽均为64，相应的标签也用随机数初始化，形状为（1,1000）。</p>
<blockquote>
<p>该示例的工作运行在CPU上，而不是GPU（即使tensor被移动到CUDA上）</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch,torchvision</span><br><span class="line"></span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>) <span class="comment"># 加载模型</span></span><br><span class="line">data = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">64</span>) <span class="comment"># 创建输入数据</span></span><br><span class="line">labels = torch.rand(<span class="number">1</span>,<span class="number">1000</span>) <span class="comment"># 随机初始化标签</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输入数据通过模型的每一层来进行预测，所谓前向传播</span></span><br><span class="line">prediction = model(data)</span><br><span class="line"><span class="comment"># 用模型预测值和标签计算误差（损失）</span></span><br><span class="line">loss = (prediction-labels).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># 使用损失，反向传播通过神经网络</span></span><br><span class="line"><span class="comment"># 在误差的tensor上使用.backward()，它会自动计算模型每一个参数的的梯度并存储在参数的.grad属性中</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># 加载优化器:随机梯度下降</span></span><br><span class="line">optim = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># 使用.step()梯度下降，优化器会根据存储在.grad中的梯度值调整每一个参数</span></span><br><span class="line">optim.step()</span><br></pre></td></tr></table></figure>

<p>以上是训练一个神经网络的步骤，下面是autograd工作的一些细节——可以随意跳过它们</p>
<hr>
<h2 id="3-autograd中的求导"><a href="#3-autograd中的求导" class="headerlink" title="3. autograd中的求导"></a>3. autograd中的求导</h2><h3 id="autograd如何收集梯度呢？"><a href="#autograd如何收集梯度呢？" class="headerlink" title="autograd如何收集梯度呢？"></a>autograd如何收集梯度呢？</h3><p>创建两个tensor：a和b，<code>requires_grad=True</code>，这表明<code>autograd</code>会追踪a和b上的每一个操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([<span class="number">6.</span>, <span class="number">4.</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>使用a和b创建另一个张量Q<br>$$<br>Q &#x3D; 3a^3 - b^2<br>$$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Q = 3*a**3 - b**2</span><br></pre></td></tr></table></figure>

<p>假设a和b是神经网络的参数，Q是损失，在训练网络时，需要计算损失的参数的梯度<br>$$<br>\frac{\partial Q}{\partial a} &#x3D; 9a^2 \<br>\frac{\partial Q}{\partial b} &#x3D; -2b<br>$$<br>执行Q.backward()，会自动计算这些梯度，同时在相应的tensor.grad属性中存储。</p>
<p>我们需要在执行<code>Q.backward()</code>时显式传递<code>gradient</code>属性，因为这里的gradient是一个<strong>向量</strong>，和Q形状相同，它表示Q的梯度<br>$$<br>\frac{dQ}{dQ}&#x3D;1<br>$$<br>同样地，可以把Q聚合成标量，然后隐式地反向传播：<code>Q.sum().backward()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">external_grad = torch.tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">Q.backward(gradient=external_grad)</span><br></pre></td></tr></table></figure>

<p>现在梯度被保存在<code>a.grad</code>和<code>b.grad</code>中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查收集到的梯度是否正确</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">9</span>*a**<span class="number">2</span> == a.grad)</span><br><span class="line"><span class="built_in">print</span>(-<span class="number">2</span>*b == b.grad</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>



<h3 id="选读：用autograd进行向量计算"><a href="#选读：用autograd进行向量计算" class="headerlink" title="选读：用autograd进行向量计算"></a>选读：用autograd进行向量计算</h3><p>在数学上，如果有一个向量值函数：$\vec y &#x3D; f(\vec x)$，用$\vec x$表示的梯度$\vec y$是雅克比矩阵(Jacobian matrix)$J$<br>$$<br>J &#x3D; (\frac{\partial y}{\partial x_1}…\frac{\partial y}{\partial x_n})&#x3D;<br>\begin{pmatrix}<br>\frac{\partial y_1}{\partial x_1} &amp; … &amp; \frac{\partial y_1}{\partial x_n} \<br>.&amp;&amp;.\.&amp;.&amp;.\.&amp;&amp;.\<br>\frac{\partial y_m}{\partial x_1} &amp; … &amp; \frac{\partial y_m}{\partial x_n}<br>\end{pmatrix}<br>$$<br>通常来说，<code>torch.autograd</code>是计算向量雅克比乘积的工具，也就是，给定一个向量$\vec v$，计算乘积$J^T<em>\vec v$。如果$\vec v$是标量函数$l&#x3D;g(\vec y)$的梯度:<br>$$<br>\vec v &#x3D; (\frac{\partial l}{\partial y_1}…\frac{\partial l}{\partial y_m})^T<br>$$<br>根据链式法则，用$\vec x$来表示$l$的梯度，即为*<em>向量-雅克比矩阵 乘积</em></em>&#x3D;&#x3D;图片未上传成功&#x3D;&#x3D;</p>
<p><img src="C:/Users/TSY/Desktop/Snipaste_2022-07-17_21-23-00.png" alt="Snipaste00"></p>
<p>向量雅可比积的这种特性使得将外部梯度馈送到具有非标量输出的模型中非常方便，也是在我们上面的例子中使用的，<code>external_grad</code>代表$\vec v$。</p>
<hr>
<h2 id="4-计算图-Computational-Graph"><a href="#4-计算图-Computational-Graph" class="headerlink" title="4. 计算图(Computational Graph)"></a>4. 计算图(Computational Graph)</h2><p>从概念上讲，<code>autograd</code>在由函数对象组成的<strong>有向无环图(DAG)<strong>中保存数据(tensor)和所有执行的操作（以及由操作产生的新tensor）。在这个DAG中，叶子节点是输入tensors，根节点是输出tensors。从根节点到叶子节点来追踪这个图，可以使用</strong>链式法则</strong>来自动计算梯度。</p>
<p>在前向传播中，autograd同时做两件事情：</p>
<ul>
<li>运行所请求的操作来计算<strong>输出tensor</strong></li>
<li>在DAG中保存操作的梯度</li>
</ul>
<p>当在DAG根节点上调用<code>.backward()</code>时，反向传播开始启动，<code>autograd</code>会做一系列操作：</p>
<ul>
<li>计算每一个<code>.grad_fn</code>的梯度</li>
<li>将它们累加到各自tensor的<code>.grad</code>属性中</li>
<li>使用链式法则，一直传播到叶子节点的tensors</li>
</ul>
<p>下图是我们示例的DAG可视化表示。在图中，箭头表示前向传播的方向，节点表示在前向传播中的每个操作的向后函数，蓝色的叶子节点表示张量a和b。</p>
<p><img src="https://pytorch.org/tutorials/_images/dag_autograd.png" alt="../../_images/dag_autograd.png"></p>
<blockquote>
<p>DAG在PyTorch中是动态的，在每一次调用.backward()之后，计算图是重新开始创建的，autograd会重新填充一个新的图，这就是允许在模型中使用控制流语句的原因。</p>
<p>可以根据需求在每次迭代时更改形状、大小和操作。</p>
</blockquote>
<h3 id="从DAG中删除-Exclusion-from-the-DAG"><a href="#从DAG中删除-Exclusion-from-the-DAG" class="headerlink" title="从DAG中删除(Exclusion from the DAG)"></a>从DAG中删除(Exclusion from the DAG)</h3><p>如果<code>requires_grad</code>设置为True，<code>torch.autograd</code>就可以追踪对tensor的相关操作。如果不需要tensor的梯度，把属性设置成False就可以从DAG的梯度计算中排除。</p>
<p>即使只有一个简单的输入tensor的<code>requires_grad=True</code>，输出tensors也需要梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">z = torch.rand((<span class="number">5</span>, <span class="number">5</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">a = x + y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Does `a` require gradients? : <span class="subst">&#123;a.requires_grad&#125;</span>&quot;</span>)</span><br><span class="line">b = x + z</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Does `b` require gradients?: <span class="subst">&#123;b.requires_grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Does `a` require gradients? : <span class="literal">False</span></span><br><span class="line">Does `b` require gradients?: <span class="literal">True</span></span><br></pre></td></tr></table></figure>



<p>在一个网络结构中，通常把不需要计算梯度的参数称为冻结参数（<strong>frozen parameters</strong>）。如果提前知道哪些参数不需要计算梯度，模型的冻结功能非常有用，还可以通过减少梯度计算来提升性能受益。</p>
<p>另一个从DAG中删除的应用是<strong>微调一个预训练的模型</strong></p>
<p>在微调的过程中，会冻结模型中的大部分，通常只修改分类层来预测新的标签。让我们通过一个小例子来说明，向以前一样，我们加载预训练模型resnet18，并且冻结所有参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 冻结网络中的所有参数</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">目的：在有10个标签的新数据集上微调模型</span></span><br><span class="line"><span class="string">resnet最后一层的线性分类器为：model.fc</span></span><br><span class="line"><span class="string">我们可以简单的使用一个新的线性层（默认未冻结）来代替它作为分类器</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">model.fc = nn.Linear(<span class="number">512</span>,<span class="number">10</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">现在除了model.fc的参数，模型的所有参数被冻结了</span></span><br><span class="line"><span class="string">只有model.fc的weights和bias来参与梯度计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<p>【注意】尽管我们在优化器中注册了所有参数，只有分类器的weights和bias计算梯度并使用梯度下降更新，<code>torch.no_grad()</code>也有同样的功能</p>
<hr>
<h2 id="5-拓展阅读"><a href="#5-拓展阅读" class="headerlink" title="5. 拓展阅读"></a>5. 拓展阅读</h2><p><strong><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/autograd.html">in-place操作&amp;多线程autograd</a></strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC">反向模式autodiff的例子</a></strong></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/07/17/tensor/" rel="prev" title="60分钟入门PyTorch——Tensor">
      <i class="fa fa-chevron-left"></i> 60分钟入门PyTorch——Tensor
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/07/18/Neural%20Nwtworks/" rel="next" title="60分钟入门PyTorch——神经网络（用PyTorch构建一个神经网络）">
      60分钟入门PyTorch——神经网络（用PyTorch构建一个神经网络） <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#torch-autograd"><span class="nav-number">1.</span> <span class="nav-text">torch.autograd</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E8%83%8C%E6%99%AF"><span class="nav-number">1.1.</span> <span class="nav-text">1. 背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%9C%A8PyTorch%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">1.2.</span> <span class="nav-text">2. 在PyTorch中的应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-autograd%E4%B8%AD%E7%9A%84%E6%B1%82%E5%AF%BC"><span class="nav-number">1.3.</span> <span class="nav-text">3. autograd中的求导</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#autograd%E5%A6%82%E4%BD%95%E6%94%B6%E9%9B%86%E6%A2%AF%E5%BA%A6%E5%91%A2%EF%BC%9F"><span class="nav-number">1.3.1.</span> <span class="nav-text">autograd如何收集梯度呢？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%89%E8%AF%BB%EF%BC%9A%E7%94%A8autograd%E8%BF%9B%E8%A1%8C%E5%90%91%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="nav-number">1.3.2.</span> <span class="nav-text">选读：用autograd进行向量计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E8%AE%A1%E7%AE%97%E5%9B%BE-Computational-Graph"><span class="nav-number">1.4.</span> <span class="nav-text">4. 计算图(Computational Graph)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8EDAG%E4%B8%AD%E5%88%A0%E9%99%A4-Exclusion-from-the-DAG"><span class="nav-number">1.4.1.</span> <span class="nav-text">从DAG中删除(Exclusion from the DAG)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB"><span class="nav-number">1.5.</span> <span class="nav-text">5. 拓展阅读</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="TT"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">TT</p>
  <div class="site-description" itemprop="description">The Journey Is the Reward</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">TT</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'TNUpeWIcsOXnq5yrEo6XphL5-gzGzoHsz',
      appKey     : 'HdE2XffT6XTWRYvV2zovg336',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
